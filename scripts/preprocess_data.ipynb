{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.model import GPTLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"TinyStories-train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path.cwd() / '..' / 'data' / 'raw' / dataset_name\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922767089\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n",
      "One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the dataset\n",
    "We will use characters as tokens, just as a baseline.\n",
    "\n",
    "Note: special tokens already exist, like `<|endoftext|>`. We'll just ignore them for now, and generate infinite text.\n",
    "\n",
    "Note: there appears to be different languages, symbols, and emojis. I can recognize Chinese characters. We'll ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{|}~¬í¬ì¬î¬†¬°¬¢¬£¬ß¬´¬≠¬∞¬¥¬∑¬ª¬ø√Ç√â√ü√†√°√¢√•√®√©√™√≠√Ø√±√≥√∂√∫ƒÅƒ∞≈ì…™ è ô ú—ñ“ì·¥Ä·¥Ñ·¥Ö·¥á·¥è·¥õ·¥ú·¥°·¥¢‚ÄÖ‚Äâ‚Ää‚Äã‚Äå‚Äé‚Äê‚Äë‚Äí‚Äì‚Äî‚Äï‚Äò‚Äô‚Äö‚Äú‚Äù‚Äû‚Ä¶‚Ä®‚Ä©‚Ä™‚Ä≤‚Ç¨‚Ñ¢‚àí‚îÄ‚ù§„ÄÄ„ÄÇ„Äç‰∏Ä‰∫Ü‰∫õ‰ªñ‰ΩÜ‰øùÂÄãÂÄëÂÖíÂÖ©ÂàÜÂà∞ÂâõÂèàÂíåÂú®Â§©Â•ÆÂ•πÂ∑±Â∑¥Â∫¶ÂæàÊÅ©ÊáâÊääÊï¥ÊòØÊôÇÊúÉÁç®ÁéâÁî∞ÁïôÁï∂ÁöÑÁ´•Á≠îÁ±≥Áµ¶Ëá™ËààËâæËéâË£°ÈÄôÈÅéÈõ£È´òÓÄÄÔÅäÔ¨ÅÔ¨ÇÔ∏èÔªøÔºåÔøºÔøΩùëêüå¥üåπüçåüçûüéìüíñüôÇü§©\n",
      "243\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerBase:\n",
    "    def encode(self, s):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def decode(self, t):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CharacterTokenizer(TokenizerBase):\n",
    "    def __init__(self, chars):\n",
    "        self.stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i, ch in enumerate(chars) }\n",
    "    \n",
    "    def encode(self, s):\n",
    "        return [self.stoi[c] for c in s]\n",
    "    \n",
    "    def decode(self, l):\n",
    "        return ''.join([self.itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 74, 74, 2, 85, 73, 70, 83, 70]\n",
      "Hii there\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(chars)\n",
    "encoded = tokenizer.encode(\"Hii there\")\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49, 79, 70,  2, 69, 66, 90, 14,  2, 66,  2, 77, 74, 85, 85, 77, 70,  2,\n",
      "        72, 74, 83, 77,  2, 79, 66, 78, 70, 69,  2, 46, 74, 77, 90,  2, 71, 80,\n",
      "        86, 79, 69,  2, 66,  2, 79, 70, 70, 69, 77, 70,  2, 74, 79,  2, 73, 70,\n",
      "        83,  2, 83, 80, 80, 78, 16,  2, 53, 73, 70,  2, 76, 79, 70, 88,  2, 74,\n",
      "        85,  2, 88, 66, 84,  2, 69, 74, 71, 71, 74, 68, 86, 77, 85,  2, 85, 80,\n",
      "         2, 81, 77, 66, 90,  2, 88, 74, 85, 73,  2, 74, 85,  2, 67, 70, 68, 66,\n",
      "        86, 84, 70,  2, 74, 85,  2, 88, 66, 84,  2, 84, 73, 66, 83, 81, 16,  2,\n",
      "        46, 74, 77, 90,  2, 88, 66, 79, 85, 70, 69,  2, 85, 80,  2, 84, 73, 66,\n",
      "        83, 70,  2, 85, 73, 70,  2, 79, 70, 70, 69, 77, 70,  2, 88, 74, 85, 73,\n",
      "         2, 73, 70, 83,  2, 78, 80, 78, 14,  2, 84, 80,  2, 84, 73, 70,  2, 68,\n",
      "        80, 86, 77, 69,  2, 84, 70, 88,  2, 66,  2, 67, 86, 85, 85, 80, 79,  2,\n",
      "        80, 79,  2, 73, 70, 83,  2, 84, 73, 74, 83, 85, 16,  1, 46, 74, 77, 90,\n",
      "         2, 88, 70, 79, 85,  2, 85, 80,  2, 73, 70, 83,  2, 78, 80, 78,  2, 66,\n",
      "        79, 69,  2, 84, 66, 74, 69, 14,  2,  4, 47, 80, 78, 14,  2, 43,  2, 71,\n",
      "        80, 86, 79, 69,  2, 85, 73, 74, 84,  2, 79, 70, 70, 69, 77, 70, 16,  2,\n",
      "        37, 66, 79,  2, 90, 80, 86,  2, 84, 73, 66, 83, 70,  2, 74, 85,  2, 88,\n",
      "        74, 85, 73,  2, 78, 70,  2, 66, 79, 69,  2, 84, 70, 88,  2, 78, 90,  2,\n",
      "        84, 73, 74, 83, 85, 33,  4,  2, 42, 70, 83,  2, 78, 80, 78,  2, 84, 78,\n",
      "        74, 77, 70, 69,  2, 66, 79, 69,  2, 84, 66, 74, 69, 14,  2,  4, 59, 70,\n",
      "        84, 14,  2, 46, 74, 77, 90, 14,  2, 88, 70,  2, 68, 66, 79,  2, 84, 73,\n",
      "        66, 83, 70,  2, 85, 73, 70,  2, 79, 70, 70, 69, 77, 70,  2, 66, 79, 69,\n",
      "         2, 71, 74, 89,  2, 90, 80, 86, 83,  2, 84, 73, 74, 83, 85, 16,  4,  1,\n",
      "        54, 80, 72, 70, 85, 73, 70, 83, 14,  2, 85, 73, 70, 90,  2, 84, 73, 66,\n",
      "        83, 70, 69,  2, 85, 73, 70,  2, 79, 70, 70, 69, 77, 70,  2, 66, 79, 69,\n",
      "         2, 84, 70, 88, 70, 69,  2, 85, 73, 70,  2, 67, 86, 85, 85, 80, 79,  2,\n",
      "        80, 79,  2, 46, 74, 77, 90,  9, 84,  2, 84, 73, 74, 83, 85, 16,  2, 43,\n",
      "        85,  2, 88, 66, 84,  2, 79, 80, 85,  2, 69, 74, 71, 71, 74, 68, 86, 77,\n",
      "        85,  2, 71, 80, 83,  2, 85, 73, 70, 78,  2, 67, 70, 68, 66, 86, 84, 70,\n",
      "         2, 85, 73, 70, 90,  2, 88, 70, 83, 70,  2, 84, 73, 66, 83, 74, 79, 72,\n",
      "         2, 66, 79, 69,  2, 73, 70, 77, 81, 74, 79, 72,  2, 70, 66, 68, 73,  2,\n",
      "        80, 85, 73, 70, 83, 16,  2, 35, 71, 85, 70, 83,  2, 85, 73, 70, 90,  2,\n",
      "        71, 74, 79, 74, 84, 73, 70, 69, 14,  2, 46, 74, 77, 90,  2, 85, 73, 66,\n",
      "        79, 76, 70, 69,  2, 73, 70, 83,  2, 78, 80, 78,  2, 71, 80, 83,  2, 84,\n",
      "        73, 66, 83, 74, 79, 72,  2, 85, 73, 70,  2, 79, 70, 70, 69, 77, 70,  2,\n",
      "        66, 79, 69,  2, 71, 74, 89, 74, 79, 72,  2, 73, 70, 83,  2, 84, 73, 74,\n",
      "        83, 85, 16,  2, 54, 73, 70, 90,  2, 67, 80, 85, 73,  2, 71, 70, 77, 85,\n",
      "         2, 73, 66, 81, 81, 90,  2, 67, 70, 68, 66, 86, 84, 70,  2, 85, 73, 70,\n",
      "        90,  2, 73, 66, 69,  2, 84, 73, 66, 83, 70, 69,  2, 66, 79, 69,  2, 88,\n",
      "        80, 83, 76, 70, 69,  2, 85, 80, 72, 70, 85, 73, 70, 83, 16,  1, 30, 93,\n",
      "        70, 79, 69, 80, 71, 85, 70, 89, 85, 93, 32,  1, 49, 79, 68, 70,  2, 86,\n",
      "        81, 80, 79,  2, 66,  2, 85, 74, 78, 70, 14,  2, 85, 73, 70, 83, 70,  2,\n",
      "        88, 66, 84,  2, 66,  2, 77, 74, 85, 85, 77, 70,  2, 68, 66, 83,  2, 79,\n",
      "        66, 78, 70, 69,  2, 36, 70, 70, 81, 16,  2, 36, 70, 70, 81,  2, 77, 80,\n",
      "        87, 70, 69,  2, 85, 80,  2, 72, 80,  2, 71, 66, 84, 85,  2, 66, 79, 69,\n",
      "         2, 81, 77, 66, 90,  2, 74, 79,  2, 85, 73, 70,  2, 84, 86, 79, 16,  2,\n",
      "        36, 70, 70, 81,  2, 88, 66, 84,  2, 66,  2, 73, 70, 66, 77, 85, 73, 90,\n",
      "         2, 68, 66, 83,  2, 67, 70, 68, 66, 86, 84, 70,  2, 73, 70,  2, 66, 77,\n",
      "        88, 66, 90, 84,  2, 73, 66, 69,  2, 72, 80, 80, 69,  2, 71, 86, 70, 77,\n",
      "        16,  2, 41, 80, 80, 69,  2, 71, 86, 70, 77,  2, 78, 66, 69, 70,  2, 36,\n",
      "        70, 70, 81,  2, 73, 66, 81, 81, 90,  2, 66, 79, 69,  2, 84, 85, 83, 80,\n",
      "        79, 72, 16,  1, 49, 79, 70,  2, 69, 66, 90, 14,  2, 36, 70, 70, 81,  2,\n",
      "        88, 66, 84,  2, 69, 83, 74, 87, 74, 79, 72,  2, 74, 79,  2, 85, 73, 70,\n",
      "         2, 81, 66, 83, 76,  2, 88, 73, 70, 79,  2, 73, 70,  2, 84, 66, 88,  2,\n",
      "        66,  2, 67, 74, 72,  2, 85, 83, 70, 70, 16,  2, 54, 73, 70,  2, 85, 83,\n",
      "        70, 70,  2, 73, 66, 69,  2, 78, 66, 79, 90,  2, 77, 70, 66, 87, 70, 84,\n",
      "         2, 85, 73, 66, 85,  2, 88, 70, 83, 70])\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "print(train_data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, context_length, batch_size, data):\n",
    "        self.context_length = context_length\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "    \n",
    "    def get_batch(self):\n",
    "        ix = torch.randint(len(self.data) - self.context_length, (self.batch_size,))\n",
    "        x = torch.stack([self.data[i:i+self.context_length] for i in ix])\n",
    "        y = torch.stack([self.data[i+1:i+self.context_length+1] for i in ix])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[72, 70, 85,  2, 73, 86, 83, 85],\n",
      "        [68, 76,  2, 84, 80, 78, 70,  2],\n",
      "        [70, 84, 85,  2, 71, 83, 74, 70],\n",
      "        [ 2, 66,  2, 88, 66, 77, 76, 16]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[70, 85,  2, 73, 86, 83, 85,  2],\n",
      "        [76,  2, 84, 80, 78, 70,  2, 85],\n",
      "        [84, 85,  2, 71, 83, 74, 70, 79],\n",
      "        [66,  2, 88, 66, 77, 76, 16,  1]])\n",
      "----\n",
      "when input is [72] the target: 70\n",
      "when input is [72, 70] the target: 85\n",
      "when input is [72, 70, 85] the target: 2\n",
      "when input is [72, 70, 85, 2] the target: 73\n",
      "when input is [72, 70, 85, 2, 73] the target: 86\n",
      "when input is [72, 70, 85, 2, 73, 86] the target: 83\n",
      "when input is [72, 70, 85, 2, 73, 86, 83] the target: 85\n",
      "when input is [72, 70, 85, 2, 73, 86, 83, 85] the target: 2\n",
      "when input is [68] the target: 76\n",
      "when input is [68, 76] the target: 2\n",
      "when input is [68, 76, 2] the target: 84\n",
      "when input is [68, 76, 2, 84] the target: 80\n",
      "when input is [68, 76, 2, 84, 80] the target: 78\n",
      "when input is [68, 76, 2, 84, 80, 78] the target: 70\n",
      "when input is [68, 76, 2, 84, 80, 78, 70] the target: 2\n",
      "when input is [68, 76, 2, 84, 80, 78, 70, 2] the target: 85\n",
      "when input is [70] the target: 84\n",
      "when input is [70, 84] the target: 85\n",
      "when input is [70, 84, 85] the target: 2\n",
      "when input is [70, 84, 85, 2] the target: 71\n",
      "when input is [70, 84, 85, 2, 71] the target: 83\n",
      "when input is [70, 84, 85, 2, 71, 83] the target: 74\n",
      "when input is [70, 84, 85, 2, 71, 83, 74] the target: 70\n",
      "when input is [70, 84, 85, 2, 71, 83, 74, 70] the target: 79\n",
      "when input is [2] the target: 66\n",
      "when input is [2, 66] the target: 2\n",
      "when input is [2, 66, 2] the target: 88\n",
      "when input is [2, 66, 2, 88] the target: 66\n",
      "when input is [2, 66, 2, 88, 66] the target: 77\n",
      "when input is [2, 66, 2, 88, 66, 77] the target: 76\n",
      "when input is [2, 66, 2, 88, 66, 77, 76] the target: 16\n",
      "when input is [2, 66, 2, 88, 66, 77, 76, 16] the target: 1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "dl_train = DataLoader(context_length=8, batch_size=4, data=train_data)\n",
    "xb, yb = dl_train.get_batch()\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(dl_train.batch_size):\n",
    "    for t in range(dl_train.context_length):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_name = 'TinyStories-valid.txt'\n",
    "val_path = Path.cwd() / '..' / 'data' / 'raw' / val_set_name\n",
    "with open(val_path, 'r') as f:\n",
    "    val_text = f.read()\n",
    "\n",
    "val_data = torch.tensor(tokenizer.encode(val_text), dtype=torch.long)\n",
    "\n",
    "dl_val = DataLoader(context_length=8, batch_size=4, data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, dl_train, dl_val, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for name, dl in [('train', dl_train), ('val', dl_val)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = dl.get_batch()\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[name] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=16,\n",
    "    seq_len=context_length,\n",
    "    n_layers=4,\n",
    "    d_k=8,\n",
    "    d_v=8,\n",
    "    n_heads=4,\n",
    "    device=device\n",
    ")\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025139 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.6260, val loss 5.6227\n",
      "step 100: train loss 3.5634, val loss 3.5697\n",
      "step 200: train loss 3.0214, val loss 3.0535\n",
      "step 300: train loss 2.8662, val loss 2.8639\n",
      "step 400: train loss 2.7410, val loss 2.7486\n",
      "step 500: train loss 2.6371, val loss 2.6446\n",
      "step 600: train loss 2.6040, val loss 2.6114\n",
      "step 700: train loss 2.5575, val loss 2.5482\n",
      "step 800: train loss 2.4675, val loss 2.4920\n",
      "step 900: train loss 2.4801, val loss 2.4892\n",
      "step 1000: train loss 2.4732, val loss 2.4239\n",
      "step 1100: train loss 2.4282, val loss 2.4311\n",
      "step 1200: train loss 2.3685, val loss 2.4133\n",
      "step 1300: train loss 2.3909, val loss 2.3778\n",
      "step 1400: train loss 2.3518, val loss 2.3900\n",
      "step 1500: train loss 2.3422, val loss 2.3148\n",
      "step 1600: train loss 2.3422, val loss 2.3081\n",
      "step 1700: train loss 2.3001, val loss 2.3426\n",
      "step 1800: train loss 2.3503, val loss 2.3204\n",
      "step 1900: train loss 2.2905, val loss 2.2734\n",
      "step 2000: train loss 2.2482, val loss 2.2306\n",
      "step 2100: train loss 2.2572, val loss 2.2353\n",
      "step 2200: train loss 2.2604, val loss 2.2962\n",
      "step 2300: train loss 2.2510, val loss 2.2707\n",
      "step 2400: train loss 2.2740, val loss 2.2393\n",
      "step 2500: train loss 2.2644, val loss 2.2001\n",
      "step 2600: train loss 2.2172, val loss 2.2032\n",
      "step 2700: train loss 2.1960, val loss 2.2129\n",
      "step 2800: train loss 2.2488, val loss 2.2046\n",
      "step 2900: train loss 2.1837, val loss 2.1808\n",
      "step 3000: train loss 2.1969, val loss 2.1735\n",
      "step 3100: train loss 2.1930, val loss 2.1850\n",
      "step 3200: train loss 2.2286, val loss 2.1831\n",
      "step 3300: train loss 2.1605, val loss 2.1789\n",
      "step 3400: train loss 2.1875, val loss 2.1953\n",
      "step 3500: train loss 2.1545, val loss 2.1614\n",
      "step 3600: train loss 2.1741, val loss 2.1398\n",
      "step 3700: train loss 2.1731, val loss 2.1641\n",
      "step 3800: train loss 2.1457, val loss 2.1387\n",
      "step 3900: train loss 2.0940, val loss 2.1305\n",
      "step 4000: train loss 2.1502, val loss 2.1369\n",
      "step 4100: train loss 2.1559, val loss 2.1415\n",
      "step 4200: train loss 2.1618, val loss 2.1115\n",
      "step 4300: train loss 2.1438, val loss 2.1163\n",
      "step 4400: train loss 2.1079, val loss 2.1303\n",
      "step 4500: train loss 2.1485, val loss 2.1254\n",
      "step 4600: train loss 2.1057, val loss 2.1037\n",
      "step 4700: train loss 2.1104, val loss 2.1147\n",
      "step 4800: train loss 2.0829, val loss 2.0715\n",
      "step 4900: train loss 2.1416, val loss 2.1193\n",
      "step 4999: train loss 2.0868, val loss 2.0480\n"
     ]
    }
   ],
   "source": [
    "eval_iters = 200\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model, dl_train, dl_val, eval_iters)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    xb, yb = dl_train.get_batch()\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\tang.\n",
      "Shey thew hite was ron said rikeans. She pether salan inchbesy nak the teu blnincgem lor sllcedote both, feche in sanaay diry anke sok aw ma bily greooo iiy fonr in. The buuir thto: es fuly wirlyes and pnekt dald sinay eve qpimed int. Shan giny, gou.\"\n",
      "Bin.\n",
      "<|e, hendole thapy ol fa inky, hip was mont a fer isted tul was Dhe meatt racig foun.\n",
      "The rut roks faen frow. Wusted conn, Si. the holp darook a and!\"Hs a a drooks. As ven nop sopit fon ske yom a and mar ig x and and hrop Than whas shink says in worghre wik be ug enbam ink tk, yom ol wanghe sain to clo\"\n",
      "Oner and sorin, saw hagan angrd\"\"\n",
      "He ou, thith shalt go thaid ay \"Oin dimu this thur omom astead oun sbut saree far, che.Iat, in sor tpichh oon wisk is fore a wut was sman goLp and wing askid nnan das cpily woane fit tham san to goy he pive. Heoors inds.\"\" The daidsude thappy, a put nukk to the shin\n",
      "Tin man, hot the the hatigis. The ry, kwis. Soy tom shere foft on'ce to yhe pit noned a to to col dand she or llors min, the not theo bit st, solp nary pok.\n",
      "He and lpom. Shad hurt op fhe that to arin, and bin a mod ge and Lin nen a han fpoares not to biysh a beve tal to brhe diry fund, golld and hain cknoft hall nou day sapon want a to pleirm fome Thally She lon dad hoa asid ley, thay yan saaw is shen wit gher ave liker. she washe dan win this ol thYo ou fuwdoghst of a prirefollay sar urcimrctpumyimet|>\n",
      "Bulile, the the thas fob. Me ol backe  sheri, g. They we oithe tu, inst thiss i'st, yound yin ilm the yoo Lin.\n",
      "Bn's.\"\"\n",
      "<|enk, yof a mer has. Her t|>\n",
      "Odigrne gil oryn.opily, ith Mplor and and sa fof wes cof gary to meo baix thi Mar stae and sfun homedhe blaiss sa and cay hinou ye fuste tad ust and a tro lored che rous.\" She caclied.\n",
      "She ou nee. Thig the wats Tand a so daid. He so wiknhe.\n",
      "Jisoad hoar Touingh pillong hop have cor sted in lwime. She fe port alced Das om witous yhay ha hiss skas in sachs ank. The hers tThirt's ocink harl me> nomus caop dinid pul. Hed So llesmaased and, put. Hery Her bike fir nou ang'eror\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 8), dtype=torch.long, device=device)\n",
    "generated = m.generate(context, max_new_tokens=2000)[0].tolist()\n",
    "decoded = tokenizer.decode(generated)\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
